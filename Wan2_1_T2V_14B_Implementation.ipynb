{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Cell 1: Installation of Project Dependencies\n",
        "This cell installs the necessary Python libraries required for the project.\n",
        "\n",
        "diffusers: A core Hugging Face library providing pre-trained diffusion models and pipelines for various modalities, including text-to-video.\n",
        "\n",
        "transformers: Provides the underlying architecture and utilities for loading and managing models from the Hugging Face Hub.\n",
        "\n",
        "accelerate: An auxiliary library from Hugging Face that optimizes PyTorch code for execution on various hardware configurations (e.g., GPUs, TPUs), ensuring efficient use of available resources.\n",
        "\n",
        "\"imageio[ffmpeg]\": A library for reading and writing a wide range of image and video data. The [ffmpeg] extra ensures that the FFmpeg backend is installed, which is a robust, open-source multimedia framework required for encoding the generated frames into a video file (e.g., MP4).\n",
        "\n",
        "av: A Pythonic binding for FFmpeg libraries, providing an alternative and sometimes more direct interface for video processing tasks.\n",
        "\n"
      ],
      "metadata": {
        "id": "bGMOcFzWqdP9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Install project dependencies via pip\n",
        "%pip install diffusers transformers accelerate \"imageio[ffmpeg]\" av"
      ],
      "metadata": {
        "id": "RKIrkzbx1H2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 2: Authentication with Hugging Face Hub\n",
        "This section handles the preliminary setup of the environment. This includes authenticating with the Hugging Face Hub to access pre-trained model weights. Access to pre-trained models on the Hugging Face Hub often requires authentication. This cell securely logs into the service using a token retrieved from Colab's secure secret manager (userdata), which is the best practice for handling sensitive credentials in a notebook environment, preventing accidental exposure."
      ],
      "metadata": {
        "id": "A7kgHfLjqhfR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Authenticate with Hugging Face Hub\n",
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "# Retrieve and use token stored in Colab's secret environment\n",
        "hf_token = userdata.get(\"HF_TOKEN\")\n",
        "login(hf_token.strip())"
      ],
      "metadata": {
        "id": "q1tlkGrl1HtJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 3: Import Required Python Libraries\n",
        "This cell imports the primary modules for the script's execution.\n",
        "\n",
        "DiffusionPipeline: A high-level abstraction from the diffusers library that encapsulates the entire generative process (from text to video).\n",
        "\n",
        "torch: The core PyTorch library, upon which the diffusion model is built. It is essential for tensor operations and GPU acceleration.\n",
        "\n",
        "numpy: A fundamental library for numerical computing in Python, used here for manipulating the video frames as numerical arrays.\n",
        "\n",
        "imageio: The library used for the final step of encoding the sequence of generated image frames into a standard video file format.\n",
        "\n",
        "IPython.display.HTML: A utility for rendering HTML content directly within the notebook, used here to display the final generated video."
      ],
      "metadata": {
        "id": "gbx3_skAqnq5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Import required Python libraries for inference and video processing\n",
        "from diffusers import DiffusionPipeline\n",
        "import torch\n",
        "import numpy as np\n",
        "import imageio\n",
        "from IPython.display import HTML"
      ],
      "metadata": {
        "id": "6y74noU31HjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 4: Initialize Text-to-Video Generation Pipeline\n",
        "This section focuses on loading the pre-trained text-to-video model and configuring it for inference. Here, we instantiate the DiffusionPipeline using a specific pre-trained model.\n",
        "\n",
        "\"cerspense/zeroscope_v2_576w\": This identifier specifies the model to be downloaded from the Hugging Face Hub. It's a version of the ZeroScope model optimized for generating videos with a width of 576 pixels.\n",
        "\n",
        "torch_dtype=torch.float32: This parameter sets the model's weights to use 32-bit floating-point precision. While float16 offers faster inference and lower memory usage, float32 provides higher numerical precision and stability. The choice represents a trade-off between performance and precision.\n",
        "\n",
        "The model is then moved to the GPU (\"cuda\") to leverage hardware acceleration, a critical step for handling the computational demands of diffusion models."
      ],
      "metadata": {
        "id": "G3RVJtuIqrcT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Initialize text-to-video generation pipeline\n",
        "pipeline = DiffusionPipeline.from_pretrained(\n",
        "    \"cerspense/zeroscope_v2_576w\",\n",
        "    torch_dtype=torch.float32  # Set model weights to 32-bit float precision\n",
        ")\n",
        "\n",
        "# Move model to GPU to enable accelerated generation\n",
        "pipeline.to(\"cuda\")"
      ],
      "metadata": {
        "id": "wk4Ss_Uv1HZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 5: User Input for Generative Prompt\n",
        "The core of the notebook resides in this section. It begins by capturing the user's creative intent via a textual prompt. The textual prompt is the primary input that conditions the diffusion model's generative process. The model will synthesize a video sequence that it interprets as corresponding to the semantic content of this prompt."
      ],
      "metadata": {
        "id": "wpD0nFqbqutE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Prompt user for video generation input\n",
        "prompt = input(\"Enter your prompt: \")  # Accepts descriptive text to guide video synthesis"
      ],
      "metadata": {
        "id": "Gei03YIb1HLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 6: Set Video Generation Parameters\n",
        "This cell defines key parameters that control the characteristics of the output video and the generation process itself.\n",
        "\n",
        "fps: Frames Per Second. Set to 8, a common rate for AI-generated video that produces reasonably smooth motion without excessive computational cost.\n",
        "\n",
        "total_frames: The total number of frames desired for the final video. The calculation (30 seconds * 8 fps) yields 240 frames.\n",
        "\n",
        "chunk_size: A critical parameter for memory management. Generating all 240 frames at once would likely cause an out-of-memory (OOM) error on most GPUs. By setting a smaller chunk size (e.g., 10 frames), we process the video in manageable batches.\n",
        "\n",
        "output_dir: A dedicated directory to store the intermediate video chunks before they are concatenated."
      ],
      "metadata": {
        "id": "csJQWDm9qyy7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Set video generation parameters\n",
        "fps = 8\n",
        "total_frames = 240  # 30 seconds * 8 fps\n",
        "chunk_size = 10     # safe batch size for memory\n",
        "output_dir = \"/content/video_chunks\"\n",
        "\n",
        "import os\n",
        "os.makedirs(output_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "O9Ygtplku7a5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 7: Generate and Save Video in Chunks\n",
        "This is the main execution loop for video synthesis. It iterates through the total frame count, generating one chunk at a time to conserve memory. A try-except block ensures that an error in one chunk does not terminate the entire process.\n",
        "\n",
        "The pipeline is executed with the following key parameters:\n",
        "\n",
        "num_inference_steps: The number of denoising steps. More steps can improve quality but increase computation time. 40 is a balanced value.\n",
        "\n",
        "guidance_scale (CFG): Controls how strictly the model adheres to the prompt. A higher value (e.g., 7.5) enforces stronger prompt alignment.\n",
        "\n",
        "The model outputs frames as float tensors, which are scaled to the standard 8-bit integer range [0, 255]. The code also includes a robust validation block to handle potential shape inconsistencies, ensuring every frame is standardized to 3-channel RGB before being saved. Finally, imageio writes the processed frames of the current chunk to an MP4 file using the efficient libx264 codec."
      ],
      "metadata": {
        "id": "RJfR7z_dq4JI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Generate and save video in chunks (final version with shape fix)\n",
        "import numpy as np\n",
        "import imageio\n",
        "\n",
        "chunk_count = total_frames // chunk_size\n",
        "\n",
        "for i in range(chunk_count):\n",
        "    print(f\"\\nGenerating chunk {i+1}/{chunk_count}...\")\n",
        "\n",
        "    try:\n",
        "        # Run pipeline — returns shape: (1, 10, 320, 320, 3)\n",
        "        video_batch = pipeline(\n",
        "            prompt,\n",
        "            num_frames=chunk_size,\n",
        "            num_inference_steps=40,\n",
        "            guidance_scale=7.5,\n",
        "            height=320,\n",
        "            width=320\n",
        "        )[0][0]  # Remove outer batch dim → shape: (10, 320, 320, 3)\n",
        "\n",
        "        # Convert batch to list of frames\n",
        "        video_frames = [frame.copy() for frame in video_batch]\n",
        "\n",
        "        # Sanity check\n",
        "        if len(video_frames) != chunk_size:\n",
        "            print(f\"⚠️ Warning: Expected {chunk_size} frames, got {len(video_frames)}\")\n",
        "            continue\n",
        "\n",
        "        frames_uint8 = []\n",
        "\n",
        "        for idx, frame in enumerate(video_frames):\n",
        "            try:\n",
        "                frame_uint8 = (frame * 255).astype(np.uint8)\n",
        "\n",
        "                # Fix grayscale or alpha issues\n",
        "                if frame_uint8.ndim == 2:\n",
        "                    frame_uint8 = np.stack([frame_uint8] * 3, axis=-1)\n",
        "                elif frame_uint8.shape[-1] > 3:\n",
        "                    frame_uint8 = frame_uint8[..., :3]\n",
        "                elif frame_uint8.shape[-1] < 3:\n",
        "                    frame_uint8 = np.repeat(frame_uint8, 3, axis=-1)\n",
        "\n",
        "                # Final shape check\n",
        "                if frame_uint8.ndim != 3 or frame_uint8.shape[-1] != 3:\n",
        "                    raise ValueError(f\"Frame {idx} has shape {frame_uint8.shape}\")\n",
        "\n",
        "                frames_uint8.append(frame_uint8)\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ Skipping frame {idx}: {e}\")\n",
        "\n",
        "        # Write this chunk to mp4\n",
        "        chunk_path = f\"{output_dir}/chunk_{i+1}.mp4\"\n",
        "        with imageio.get_writer(chunk_path, fps=fps, codec=\"libx264\") as writer:\n",
        "            for frame in frames_uint8:\n",
        "                writer.append_data(frame)\n",
        "\n",
        "        print(f\"✅ Saved chunk {i+1} with {len(frames_uint8)} frames\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Skipping chunk {i+1} due to error: {e}\")\n"
      ],
      "metadata": {
        "id": "pEgSSEbM1G-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 8: Create a Concatenation File List for FFmpeg\n",
        "With all the video chunks generated, the final step is to merge them. FFmpeg's concat demuxer requires a manifest file that lists all the input files to be joined in the correct order. This cell programmatically generates that chunks_list.txt file.\n",
        "\n"
      ],
      "metadata": {
        "id": "7L-SdU1tq8dl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: Create a list of chunk files for ffmpeg\n",
        "with open(\"chunks_list.txt\", \"w\") as f:\n",
        "    for i in range(chunk_count):\n",
        "        f.write(f\"file '{output_dir}/chunk_{i+1}.mp4'\\n\")\n"
      ],
      "metadata": {
        "id": "5YcoaX9Lv83s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 9: Merge Video Chunks with FFmpeg\n",
        "This cell executes a shell command to call the FFmpeg utility.\n",
        "\n",
        "-f concat: Specifies the concatenation protocol.\n",
        "\n",
        "-safe 0: A necessary flag to allow concatenation of files with paths as specified in our list file.\n",
        "\n",
        "-c copy: This is a crucial optimization. It tells FFmpeg to perform a stream copy of the video data without re-encoding. This is extremely fast and preserves the original quality of the chunks.\n",
        "\n",
        "/content/final_output.mp4: The path for the final, merged video."
      ],
      "metadata": {
        "id": "iwFqOtyKrAFZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9: Merge all chunks into one video using ffmpeg\n",
        "!ffmpeg -f concat -safe 0 -i chunks_list.txt -c copy /content/final_output.mp4\n"
      ],
      "metadata": {
        "id": "ittjcXdiwIWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 10: Note on Redundant Code Block\n",
        "The code in this cell appears to be from a previous workflow version, as it processes a variable video_frames that now only holds the last processed chunk. The primary workflow already saves chunks in Cell 7 and merges them in Cell 9. This block is not part of the main chunking-and-merging pipeline but could be used for quickly previewing a single chunk without the full merge process.\n",
        "\n"
      ],
      "metadata": {
        "id": "s5AMtfehrBX7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 10: Convert float32 frames to 8-bit format and encode to MP4\n",
        "video_path = \"/content/Wan2.1-T2V-14B_output1.mp4\"\n",
        "\n",
        "# Extract frames and scale from [0.0, 1.0] to [0, 255] as uint8 for encoding\n",
        "frames_np    = np.array(video_frames)[0]  # Only process the first video in batch\n",
        "frames_uint8 = (frames_np * 255).astype(np.uint8)\n",
        "\n",
        "# Encode frames into a video using imageio with H.264 codec\n",
        "with imageio.get_writer(video_path, fps=8, codec=\"libx264\") as writer:\n",
        "    for frame in frames_uint8:\n",
        "        writer.append_data(frame)"
      ],
      "metadata": {
        "id": "o1FGh7EG1Gw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 11: Display Rendered Video Inline\n",
        "This cell uses IPython's HTML rendering capabilities to embed an HTML5 <video> tag directly into the notebook's output. This allows for immediate playback and review of the final generated video without needing to download it first. The src path points to the final, merged video."
      ],
      "metadata": {
        "id": "ihuNYRf0rEBG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 11: Display rendered video inline using HTML5 video tag\n",
        "HTML(f\"\"\"\n",
        "<video width=\"256\" height=\"256\" controls autoplay loop>\n",
        "  <source src=\"{video_path}\" type=\"video/mp4\">\n",
        "  Your browser does not support the video tag.\n",
        "</video>\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "d3RDfS_u1GiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 12: Provide Download Link for Final Video\n",
        "This cell leverages a Google Colab-specific module (google.colab.files) to create and trigger a browser-based download of the final video file. This provides a convenient method for saving the artifact locally."
      ],
      "metadata": {
        "id": "vJCx3nUzrIKK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 12: Provide download link for final video\n",
        "from google.colab import files\n",
        "files.download(\"/content/final_output.mp4\")\n"
      ],
      "metadata": {
        "id": "5gRr-2NacSje"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}